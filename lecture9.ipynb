{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66020868-44a1-44ea-b9bc-5d1bbfde217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|             country|avg_age|\n",
      "+--------------------+-------+\n",
      "|                Chad|  36.25|\n",
      "|            Paraguay|  47.78|\n",
      "|            Anguilla|   72.0|\n",
      "|               Macao|   72.0|\n",
      "|Heard Island and ...|   30.0|\n",
      "|             Senegal|   53.0|\n",
      "|              Sweden|  45.33|\n",
      "|             Tokelau|  34.17|\n",
      "|French Southern T...|  50.67|\n",
      "|            Kiribati|  48.67|\n",
      "|   Republic of Korea|  58.17|\n",
      "|              Guyana|   39.0|\n",
      "|             Eritrea|  39.75|\n",
      "|              Jersey|   58.8|\n",
      "|         Philippines|  48.33|\n",
      "|            Djibouti|   38.6|\n",
      "|               Tonga|   49.0|\n",
      "|      Norfolk Island|  35.33|\n",
      "|            Malaysia|  60.67|\n",
      "|           Singapore|   40.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    avg,\n",
    "    col,\n",
    "    round as rnd\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sql_import_csv\").getOrCreate()\n",
    "csv_file_path = \"file:///home/jovyan/work/sample/age.csv\"\n",
    "\n",
    "# header option: either csv has header or not(default: header = false)\n",
    "# inferSchema: either all columns are str or not\n",
    "# csv import를 위해서 read를 먼저 호출해주고 여러 옵션을 주어야 함\n",
    "# header true를 주면 스파크의 테이블이 자동으로 컬럼 네임으로 됨\n",
    "# inferSchema는 false를 주면 모든 데이터들이 string으로 import됨 다만 age의 경우에는 number이니까 숫자의 경우 자동으로 number로 로드됨\n",
    "data = spark.read.option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .csv(csv_file_path)\n",
    "# data = spark.read.option(\"header\", \"true\")\\\n",
    "#             .csv(csv_file_path)\n",
    "\n",
    "# # show schema\n",
    "# 스키마의 정보를 보여줌\n",
    "# data.printSchema()\n",
    "\n",
    "# # show column name with data\n",
    "# 컬럼의 데이터를 볼 수 있음\n",
    "# data.select(\"name\", \"age\").orderBy(col(\"age\").desc()).show()\n",
    "\n",
    "# # filter the data for age of 20 above\n",
    "# data.filter(data.age > 20).show()\n",
    "\n",
    "# # group by age and aggregates for count\n",
    "# data.groupBy(\"age\").count().show()\n",
    "\n",
    "# # custom arithmetic\n",
    "# data.select(data.name, data.age, data.age - 10).show()\n",
    "\n",
    "# # column alias\n",
    "# data.select(data.name, col(\"age\").alias(\"age1\")).show()\n",
    "\n",
    "# # average\n",
    "# data.select(data.name, data.age, data.country)\\\n",
    "#         .groupBy(\"country\")\\\n",
    "#         .avg(\"age\").show()\n",
    "\n",
    "# # average & sort\n",
    "# data.select(data.name, data.age, data.country)\\\n",
    "#         .groupBy(\"country\")\\\n",
    "#         .avg(\"age\").sort(\"avg(age)\").show()\n",
    "\n",
    "# # average & round\n",
    "# agg는 aggregation == 일종의 집계 평균 구한뒤 반올림 해서 두번째 자리까지 표현\n",
    "data.select(data.name, data.age, data.country)\\\n",
    "        .groupBy(\"country\")\\\n",
    "        .agg(rnd(avg(\"age\"), 2).alias(\"avg_age\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
